% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/smooth.R
\name{smooth.spline.funs}
\alias{smooth.spline.funs}
\title{Smoothing spline training and prediction functions, and special leave-one-out
  fitting function.}
\usage{
smooth.spline.funs(df = NULL, spar = NULL, cv = TRUE, tol.fact = 1e-06)
}
\arguments{
\item{df}{Desired effective degrees of freedom of the smoothing spline fit.
If NULL, then the complexity of the fit controlled either the spar or cv
arguments (see below).}

\item{spar}{Parameter that scales roughly as log(lambda), where lambda is
the parameter multiplying the roughness penalty in the smoothing spline
criterion. See the smooth.spline help file for details. If NULL, then the
complexity of the fit is chosen by the cv argument (see next).}

\item{cv}{Either TRUE, indicating that leave-one-out cross-validation should
be used to choose the smoothing parameter lambda, or FALSE, indicating that
generalized cross-validation should be used to choose lamdbda. Note that
this argument is only in effect when both df and spar are NULL.}

\item{tol.fact}{Factor multiplying the interquartile range of the x values,
IQR(x), to determine a threshold for uniqueness: the x values are binned
into bins of size tol.fact * IQR(x), and values which fall into the same
bin are regarded as the same. Default is 1e-6.}
}
\value{
A list with four components: train.fun, predict.fun, special.fun,
  active.fun. The last function is designed to take the output of train.fun,
  and reports which features are active for each fitted model contained in
  this output. Trivially, here, there is only one feature and it is always
  active.
}
\description{
Construct the training and prediction functions for smoothing splines, at a
  particular tuning parameter value or df value (either given or chosen by
  cross-validation), and construct also a special function for computing
  leave-one-out fitted values. Based on the \code{\link{smooth.spline}}
  function.
}
\details{
Note that the constructed function special.fun only serve as an
  approximation to the exact leave-one-out fits when the smooth.spline
  function merges nearby adjacent x points (which happens when adjacent x
  points are closer than tol), since in this case the smooth.spline function
  only returns the leverage scores (diagonal elements of the smoother matrix)
  on the reduced (merged) data set.  Another important note is that the
  constructed special.fun will be formally invalid when cross-validation (or
  generalized cross-validation) is used to pick the smooth parameter. In
  both of these cases, special.fun does \emph{not} throw a warning, and it is
  left up to the user to consider using this function wisely.
}
\examples{
## Smoothing splines: some simple univariate examples

# Generate some example training data, clearly heteroskedastic
set.seed(33)
n = 1000
x = runif(n,0,2*pi)
y = sin(x) + x*pi/30*rnorm(n)

# Generate some example test data
n0 = 1000
x0 = runif(n,0,2*pi)
y0 = sin(x0) + x0*pi/30*rnorm(n0)

# Smoothing spline training and prediction functions, where the smoothing
# parameter is chosen via cross-validation
funs = smooth.spline.funs(cv=TRUE)

# Jackknife conformal for out-of-sample prediction intervals (valid on average
# over new x0 values), we can use the special leave-one-out fitting function
out.jack = conformal.pred.jack(x, y, x0, alpha=0.1,
  train.fun=funs$train, predict.fun=funs$predict, special.fun=funs$special)

plot(c(), c(), xlab="x0", ylab="y0", xlim=range(x0),
     ylim=range(c(y0,out.jack$lo,out.jack$up)), col="white",     
     main=paste0("Jackknife (out-of-sample) prediction intervals\n",
       sprintf("Average length: \%0.3f",mean(out.jack$up-out.jack$lo))))
o = order(x0)
segments(x0[o], out.jack$lo[o], x0[o], out.jack$up[o], col="pink")
lines(x0[o], out.jack$pred[o], lwd=2, col="red")
points(x0, y0, col="gray50")

# Split conformal for out-of-sample prediction intervals (valid on average over
# new x0 values)
out.split = conformal.pred.split(x, y, x0, alpha=0.1,
  train.fun=funs$train, predict.fun=funs$predict)

plot(c(), c(), xlab="x0", ylab="y0", xlim=range(x0),
     ylim=range(c(y0,out.split$lo,out.split$up)), col="white",     
     main=paste0("Split conformal (out-of-sample) prediction intervals\n",
       sprintf("Average length: \%0.3f",mean(out.split$up-out.split$lo))))
o = order(x0)
segments(x0[o], out.split$lo[o], x0[o], out.split$up[o], col="lightgreen")
lines(x0[o], out.split$pred[o], lwd=2, col="darkgreen")
points(x0, y0, col="gray50")

# Rank-one-out split conformal for in-sample prediction intervals (valid on 
# averaged over observed x values)
out.roo = conformal.pred.roo(x, y, alpha=0.1,
  train.fun=funs$train, predict.fun=funs$predict)

plot(c(), c(), xlab="x", ylab="y", xlim=range(x),
     ylim=range(c(y,out.roo$lo,out.roo$up)), col="white",
     main=paste0("ROO-conformal (in-sample) prediction intervals\n",
       sprintf("Average length: \%0.3f",mean(out.roo$up-out.roo$lo))))
o = order(x)
segments(x[o], out.roo$lo[o], x[o], out.roo$up[o], col="lightblue")
lines(x[o], out.roo$fit.all[o], lwd=2, col="blue")
points(x, y, col="gray50")

## Adjust now for heteroskedasticity, by using mad.train.fun and mad.predict.fun

# Jackknife conformal is much slower to run, because we can't use the special
# leave-one-out fitting function anymore
out.jack.local = conformal.pred.jack(x, y, x0, alpha=0.1,
  train.fun=funs$train, predict.fun=funs$predict,
  mad.train.fun=funs$train, mad.predict.fun=funs$predict, verb=TRUE)

plot(c(), c(), xlab="x0", ylab="y0", xlim=range(x0),
     ylim=range(c(y0,out.jack.local$lo,out.jack.local$up)), col="white",     
     main=paste0("Jackknife (out-of-sample) prediction intervals\n",
       sprintf("Average length: \%0.3f",
               mean(out.jack.local$up-out.jack.local$lo))))
o = order(x0)
segments(x0[o], out.jack.local$lo[o], x0[o], out.jack.local$up[o], col="pink")
lines(x0[o], out.jack.local$pred[o], lwd=2, col="red")
points(x0, y0, col="gray50")

# Split conformal, also using smooth splines (under CV) to train on residuals
out.split.local = conformal.pred.split(x, y, x0, alpha=0.1,
  train.fun=funs$train, predict.fun=funs$predict,
  mad.train.fun=funs$train, mad.predict.fun=funs$predict)

plot(c(), c(), xlab="x0", ylab="y0", xlim=range(x0), 
     ylim=range(c(y0,out.split.local$lo,out.split.local$up)), col="white",     
     main=paste0("Split conformal (out-of-sample) prediction intervals\n",
       sprintf("Average length: \%0.3f",
               mean(out.split.local$up-out.split.local$lo))))
o = order(x0)
segments(x0[o], out.split.local$lo[o], x0[o], out.split.local$up[o],
         col="lightgreen")
lines(x0[o], out.split.local$pred[o], lwd=2, col="darkgreen")
points(x0, y0, col="gray50")

# Rank-one-out split conformal, again using smoothing splines (under CV) to
# train on residuals
out.roo.local = conformal.pred.roo(x, y, alpha=0.1,
  train.fun=funs$train, predict.fun=funs$predict,
  mad.train.fun=funs$train, mad.predict.fun=funs$predict)

plot(c(), c(), xlab="x", ylab="y", xlim=range(x),
     ylim=range(c(y,out.roo.local$lo,out.roo.local$up)), col="white",
     main=paste0("ROO-conformal (in-sample) prediction intervals\n",
       sprintf("Average length: \%0.3f",
               mean(out.roo.local$up-out.roo.local$lo))))
o = order(x)
segments(x[o], out.roo.local$lo[o], x[o], out.roo.local$up[o], col="lightblue")
lines(x[o], out.roo.local$fit.all[o], lwd=2, col="blue")
points(x, y, col="gray50")

# The reason the local from rank-one-out method here don't look entirely smooth
# is because they're actually a mishmash of two separate split procedures on
# each half of the data set (so they'd look smooth over the points within each
# half, but not necessarily overall)
}
