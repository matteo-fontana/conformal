% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/loco.R
\name{loco}
\alias{loco}
\title{Variable importance via mean (or median) excess test error}
\usage{
loco(
  x,
  y,
  train.fun,
  predict.fun,
  active.fun,
  alpha = 0.1,
  bonf.correct = TRUE,
  split = NULL,
  seed = NULL,
  verbose = FALSE
)
}
\arguments{
\item{x}{Matrix of features, of dimension (say) n x p.}

\item{y}{Vector of responses, of length (say) n.}

\item{train.fun}{A function to perform model training, i.e., to produce an
estimator of E(Y|X), the conditional expectation of the response variable
Y given features X. Its input arguments should be x: matrix of features,
and y: vector of responses.}

\item{predict.fun}{A function to perform prediction for the (mean of the)
responses at new feature values. Its input arguments should be out: output
produced by train.fun, and newx: feature values at which we want to make
predictions.}

\item{active.fun}{A function which takes the output of train.fun, and reports
which features are active for each fitted model contained in this output.
Its only input argument should be out: output produced by train.fun.}

\item{alpha}{Miscoverage level for the confidence intervals, i.e., intervals
with coverage 1-alpha are formed. Default for alpha is 0.1.}

\item{bonf.correct}{Should a Bonferroni correction be applied to the p-values
and confidence intervals? Default is TRUE.}

\item{split}{Indices that define the data-split to be used (i.e., the indices
define the first half of the data-split, on which the model is trained).
Default is NULL, in which case the split is chosen randomly.}

\item{seed}{Integer to be passed to set.seed before defining the random 
data-split to be used. Default is NULL, which effectively sets no seed.
If both split and seed are passed, the former takes priority and the latter
is ignored.}

\item{verbose}{Should intermediate progress be printed out? Default is FALSE.}
}
\value{
A list with the following components: inf.z, inf.sign, inf.wilcox,
  active, master, bonf.correct. The first three are lists, containing the
  results of LOCO inference with the Z-test, sign text, and Wilcoxon signed
  rank test, respectively. More details on these tests are given below. These
  lists have one element per tuning step inherent to the training and
  prediction functions, train.fun and predict.fun. The fourth returned
  component active is a list, with one element per tuning step, that reports
  which features are active in the corresponding fitted model. The fifth
  returned component master collects all active features across all tuning
  steps, for easy reference. The last component signals whether a Bonferroni
  correction has been applied.
}
\description{
Compute confidence intervals for mean (or median) excess test error due to 
  dropping a variable
}
\details{
In leave-one-covariate-out or LOCO inference, the training data is
  split in two parts, and the first part is used to train a model, or some
  number of models across multiple tuning steps (e.g., indexed by different 
  tuning parameter values lambda in the lasso, or different steps along the
  forward stepwise path). For each model, each variable is left out one at
  time from the first part of the data, and the entire training procedure is
  repeated without this variable in consideration. Residuals are computed on
  both from the original fitted model, and the refitted model without the
  variable in consideration. A pairwise difference between the latter and
  former residuals is computed, and either a Z-test, sign test, or Wilcoxon
  signed rank test is performed to test either the mean or median difference
  here being zero. The Z-test is of course approximately valid under the
  conditions needed for the CLT; the sign test is distribution-free and exact
  in finite samples (only assumes continuity of the underlying distribution);
  the Wilcoxon signed rank test is also distribution-free and exact in finite
  samples, but may offer more power (it assumes both continuity and symmetry
  of the underlying distribution).

  A couple other important notes: p-values here are from a one-sided test of
  the target parameter (mean or median excess test error) being equal to zero
  versus greater than zero. Confidence intervals are from inverting the
  two-sided version of this test.
}
\examples{
## Use lasso + CV to choose a model, then test variable importance in the
## selected model

# Generate some example training data
set.seed(33)
n = 200; p = 500; s = 5
x = matrix(rnorm(n*p),n,p)
beta = 2*c(rnorm(s),rep(0,p-s)) 
y = x \%*\% beta + rnorm(n)

# Lasso training and prediction functions, using cross-validation over 100
# values of lambda, with the 1se rule
funs = lasso.funs(nlambda=100,cv=TRUE,cv.rule="1se")

# Split-sample LOCO analysis
out.loco = loco(x, y, alpha=0.1, train.fun=funs$train, predict.fun=funs$predict,
  active.fun=funs$active.fun, verbose=TRUE)
out.loco

# Plot the Wilcoxon intervals
ylim = range(out.loco$inf.wilcox[[1]][,2:3])
J = length(out.loco$active[[1]])
plot(c(), c(), xlim=c(1,J), ylim=ylim, xaxt="n",
     main="LOCO analysis from lasso + CV model",
     xlab="Variable", ylab="Confidence interval")
axis(side=1, at=1:J, labels=FALSE)
for (j in 1:J) {
  axis(side=1, at=j, labels=out.loco$active[[1]][j], cex.axis=0.75,
       line=0.5*j\%\%2)
}
abline(h=0)
segments(1:J, out.loco$inf.wilcox[[1]][,2],
         1:J, out.loco$inf.wilcox[[1]][,3],
         col="red", lwd=2)
}
\references{
"Distribution-Free Predictive Inference for Regression" by Lei,
  G'Sell, Rinaldo, Tibshirani, Wasserman (2018).
}
