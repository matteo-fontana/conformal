% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/glmnet.R
\name{glmnet.funs}
\alias{glmnet.funs}
\alias{elastic.funs}
\alias{lasso.funs}
\alias{ridge.funs}
\title{Elastic net, lasso, ridge regression training and prediction functions.}
\usage{
elastic.funs(
  gamma = 0.5,
  family = c("gaussian", "binomial", "poisson", "multinomial", "cox", "mgaussian"),
  standardize = TRUE,
  intercept = TRUE,
  lambda = NULL,
  nlambda = 50,
  lambda.min.ratio = 1e-04,
  cv = FALSE,
  cv.rule = c("min", "1se")
)

lasso.funs(
  standardize = TRUE,
  intercept = TRUE,
  lambda = NULL,
  family = c("gaussian", "binomial", "poisson", "multinomial", "cox", "mgaussian"),
  nlambda = 50,
  lambda.min.ratio = 1e-04,
  cv = FALSE,
  cv.rule = c("min", "1se")
)

ridge.funs(
  standardize = TRUE,
  intercept = TRUE,
  lambda = NULL,
  family = c("gaussian", "binomial", "poisson", "multinomial", "cox", "mgaussian"),
  nlambda = 50,
  lambda.min.ratio = 1e-04,
  cv = FALSE,
  cv.rule = c("min", "1se")
)
}
\arguments{
\item{gamma}{Mixing parameter (between 0 and 1) for the elastic net, where
0 corresponds to ridge regression, and 1 to the lasso. Default is 0.5.}

\item{standardize, intercept}{Should the data be standardized, and should an
intercept be included? Default for both is TRUE.}

\item{lambda}{Sequence of lambda values over which training is performed.
This must be in decreasing order, and --- this argument should be used with
caution! When used, it is usually best to grab the sequence constructed by
one intial call to glmnet (see examples). Default is NULL, which means that
the nlambda, lambda.min.ratio arguments will define the lambda sequence
(see next).}

\item{nlambda}{Number of lambda values over which training is performed. In
particular, the lambda sequence is defined by nlambda log-spaced values
between lambda.max and lambda.min.ratio * lambda.max, where lambda.max is
the smallest value of lambda at which the solution has all zero components,
and lambda.min.ratio is a small fraction (see next). Default is 50.}

\item{lambda.min.ratio}{Small fraction that gets used in conjunction with
nlambda to specify a lambda sequence (see above). Default is 1e-4.}

\item{cv}{Should 10-fold cross-validation be used? If TRUE, then prediction
is done with either the (usual) min rule, or the 1se rule. See the cv.rule
argument, below. If FALSE (the default), then prediction is done over the
same lambda sequence as that used for training.}

\item{cv.rule}{If the cv argument is TRUE, then cv.rule determines which rule
should be used for the predict function, either "min" (the usual rule) or
"1se" (the one-standard-error rule). See the glmnet help files for details.
Default is "min".}
}
\value{
A list with three components: train.fun, predict.fun, active.fun.
  The third function is designed to take the output of train.fun, and
  reports which features are active for each fitted model contained in
  this output.
}
\description{
Construct training and prediction functions for the elastic net, the lasso,
  or ridge regression, based on the \code{\link{glmnet}} package, over a 
  sequence of (given or internally computed) lambda values.
}
\details{
This function is based on the packages \code{\link{glmnet}} and
  \code{\link{plyr}}. If these packages are not installed, then the function
  will abort. The functions lasso.funs and ridge.funs are convenience
  functions, they simply call elastic.funs with gamma = 1 and gamma = 0,
  respectively.
}
\examples{
## Elastic net: use a fixed sequence of 100 lambda values

# Generate some example training data
set.seed(11)
n = 200; p = 500; s = 10
x = matrix(rnorm(n*p),n,p)
beta = c(rnorm(s),rep(0,p-s)) 
y = x \%*\% beta + rnorm(n)

# Generate some example test data
n0 = 1000
x0 = matrix(rnorm(n0*p),n0,p)
y0 = x0 \%*\% beta + rnorm(n0)

# Grab a fixed lambda sequence from one call to glmnet, then
# define elastic net training and prediction functions
if (!require("glmnet",quietly=TRUE)) {
  stop("Package glmnet not installed (required for this example)!") 
}
out.gnet = glmnet(x,y,alpha=0.9,nlambda=100,lambda.min.ratio=1e-4)
lambda = out.gnet$lambda
funs = elastic.funs(gamma=0.9,lambda=lambda)

# Split conformal inference 
out.split = conformal.pred.split(x, y, x0, alpha=0.1,
  train.fun=funs$train, predict.fun=funs$predict)

y0.mat = matrix(rep(y0,ncol(out.split$lo)),nrow=n0)
cov.split = colMeans(out.split$lo <= y0.mat & y0.mat <= out.split$up)
len.split = colMeans(out.split$up - out.split$lo)
err.split = colMeans((y0.mat - out.split$pred)^2)

# Compare to parametric intervals from oracle linear regression
lm.orac = lm(y~x[,1:s])
out.orac = predict(lm.orac, list(x=x0[,1:s]),
  interval="predict", level=0.9)

cov.orac = mean(out.orac[,"lwr"] <= y0 & y0 <= out.orac[,"upr"])
len.orac = mean(out.orac[,"upr"] - out.orac[,"lwr"])
err.orac = mean((y0 - out.orac[,"fit"])^2)
  
# Plot average coverage 
plot(log(lambda),cov.split,type="o",pch=20,ylim=c(0,1),
     xlab="log(lambda)",ylab="Avg coverage",
     main=paste0("Split conformal + elastic net (fixed lambda sequence):",
       "\nAverage coverage"))
abline(h=cov.orac,lty=2,col=2)
legend("bottomleft",col=1:2,lty=1:2,
       legend=c("Split conformal","Oracle"))

# Plot average length
plot(log(lambda),len.split,type="o",pch=20,
     ylim=range(len.split,len.orac),
     xlab="log(lambda)",ylab="Avg length",
     main=paste0("Split conformal + elastic net (fixed lambda sequence):",
       "\nAverage length"))
abline(h=len.orac,lty=2,col=2)
legend("topleft",col=1:2,lty=1:2,
       legend=c("Split conformal","Oracle"))

# Plot test error
plot(log(lambda),err.split,type="o",pch=20,
     ylim=range(err.split,err.orac),
     xlab="log(lambda)",ylab="Test error",
     main=paste0("Split conformal + elastic net (fixed lambda sequence):",
       "\nTest error"))
abline(h=err.orac,lty=2,col=2)
legend("topleft",col=1:2,lty=1:2,
       legend=c("Split conformal","Oracle"))

####################

cat("Type return to continue ...\n")
tmp = readLines(n=1)

## Elastic net: alternate way, use a dynamic sequence of 100 lambda values

# Elastic net training and prediction functions
funs = elastic.funs(gamma=0.9,nlambda=100)

# Split conformal inference 
out.split = conformal.pred.split(x, y, x0, alpha=0.1,
  train.fun=funs$train, predict.fun=funs$predict)

y0.mat = matrix(rep(y0,ncol(out.split$lo)),nrow=n0)
cov.split = colMeans(out.split$lo <= y0.mat & y0.mat <= out.split$up)
len.split = colMeans(out.split$up - out.split$lo)
err.split = colMeans((y0.mat - out.split$pred)^2)

# Compare to parametric intervals from oracle linear regression
lm.orac = lm(y~x[,1:s])
out.orac = predict(lm.orac, list(x=x0[,1:s]),
  interval="predict", level=0.9)

cov.orac = mean(out.orac[,"lwr"] <= y0 & y0 <= out.orac[,"upr"])
len.orac = mean(out.orac[,"upr"] - out.orac[,"lwr"])
err.orac = mean((y0 - out.orac[,"fit"])^2)
  
# Plot average coverage 
plot(log(length(cov.split):1),cov.split,type="o",pch=20,,ylim=c(0,1),
     xlab="log(lambda rank) (i.e., log(1)=0 is smallest)",
     ylab="Avg coverage",
     main=paste0("Split conformal + elastic net (dynamic lambda sequence):",
       "\nAverage coverage"))
abline(h=cov.orac,lty=2,col=2)
legend("bottomleft",col=1:2,lty=1:2,
       legend=c("Split conformal","Oracle"))

# Plot average length
plot(log(length(len.split):1),len.split,type="o",pch=20,
     ylim=range(len.split,len.orac),
     xlab="log(lambda rank) (i.e., log(1)=0 is smallest)",
     ylab="Avg length",
     main=paste0("Split conformal + elastic net (dynamic lambda sequence):",
       "\nAverage length"))
abline(h=len.orac,lty=2,col=2)
legend("topleft",col=1:2,lty=1:2,
       legend=c("Split conformal","Oracle"))

# Plot test error
plot(log(length(err.split):1),err.split,type="o",pch=20,
     ylim=range(err.split,err.orac),
     xlab="log(lambda rank) (i.e., log(1)=0 is smallest)",ylab="Test error",
     main=paste0("Split conformal + elastic net (dynamic lambda sequence):",
       "\nTest error"))
abline(h=err.orac,lty=2,col=2)
legend("topleft",col=1:2,lty=1:2,
       legend=c("Split conformal","Oracle"))

####################

cat("Type return to continue ...\n")
tmp = readLines(n=1)

## Elastic net: cross-validate over a sequence of 100 lambda values

# Elastic net training and prediction functions
funs = elastic.funs(gamma=0.9,nlambda=100,cv=TRUE)

# Split conformal inference 
out.split = conformal.pred.split(x, y, x0, alpha=0.1,
  train.fun=funs$train, predict.fun=funs$predict)

cov.split = mean(out.split$lo <= y0 & y0 <= out.split$up)
len.split = mean(out.split$up - out.split$lo)
err.split = mean((y0 - out.split$pred)^2)

# Compare to parametric intervals from oracle linear regression
lm.orac = lm(y~x[,1:s])
out.orac = predict(lm.orac, list(x=x0[,1:s]),
  interval="predict", level=0.9)

cov.orac = mean(out.orac[,"lwr"] <= y0 & y0 <= out.orac[,"upr"])
len.orac = mean(out.orac[,"upr"] - out.orac[,"lwr"])
err.orac = mean((y0 - out.orac[,"fit"])^2)

tab = matrix(c(cov.split,len.split,err.split,
  cov.orac,len.orac,err.orac),ncol=2)
colnames(tab) = c("Split conformal","Oracle")
rownames(tab) = c("Avg coverage","Avg length","Test error")
tab
}
\seealso{
\code{\link{lars.funs}} for stepwise, least angle regression, and
  lasso path training and prediction functions.
}
